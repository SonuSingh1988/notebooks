{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BQ Pipeline.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"ESTHNsH4TxDd","colab_type":"text"},"source":["# BigQuery Pipeline\n","Google Cloud Datalab, with the *pipeline* subcommand, enables productionizing (i.e. scheduling and orchestrating) notebooks that accomplish ETL with BigQuery and GCS. \n","\n"],"cell_type":"markdown"},{"metadata":{"id":"_sabjhoUVGkG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/"},"outputId":"a46a6f76-45c0-4aed-ee5c-675f38ef0768","executionInfo":{"status":"ok","timestamp":1516934790398,"user_tz":480,"elapsed":1233,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["%%bq pipeline --help"],"cell_type":"code","execution_count":38,"outputs":[{"output_type":"stream","text":["usage: %bq pipeline [-h] -n NAME [-d GCS_DAG_BUCKET] [-f GCS_DAG_FILE_PATH]\n","\n","Creates a GCS/BigQuery ETL pipeline. The cell-body is specified as follows:\n","  input:\n","    table | path: <BQ table name or GCS path; both if path->table load is also required>\n","    schema: <For syntax, refer '%%bq execute'>\n","    format: {csv (default) | json}\n","    csv: <This section is relevant only when 'format' is 'csv'>\n","      delimiter: <The field delimiter to use; default is ','>\n","      skip: <Number of rows at the top of a CSV file to skip; default is 0>\n","      strict: <{True | False (default)}; whether to accept rows with missing trailing (or optional) columns>\n","      quote: <Value used to quote data sections; default is '\"'>\n","    mode: <{append (default) | overwrite}; required if path->table load>\n","  transformation: <optional; when absent, a direct conversion is done from input (path|table) to output (table|path)>\n","    query: <name of BQ query defined via \"%%bq query --name ...\">\n","  output:\n","    table | path: <BQ table name or GCS path; both if table->path extract is required>\n","    format: <{csv (default) | json}>\n","    csv: <This section is relevant only when 'format' is 'csv'>\n","      delimiter: <the field delimiter to use. Defaults to ','>\n","      header: <{True (default) | False}; Whether to include an initial header line>\n","      compress: <{True | False (default) }; Whether to compress the data on export>\n","  schedule:\n","    start: <formatted as '%Y-%m-%dT%H:%M:%S'; default is 'now'>\n","    end:  <formatted as '%Y-%m-%dT%H:%M:%S'; default is 'forever'>\n","    interval: <{@once (default) | @hourly | @daily | @weekly | @ monthly | @yearly | <cron ex>}>\n","    catchup: <{True | False (default)}; Whether to have pipelines execute in the past>\n","  parameters: <For syntax, refer '%%bq execute'>\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  -n NAME, --name NAME  BigQuery pipeline name\n","  -d GCS_DAG_BUCKET, --gcs_dag_bucket GCS_DAG_BUCKET\n","                        The Google Cloud Storage bucket for the Airflow dags.\n","  -f GCS_DAG_FILE_PATH, --gcs_dag_file_path GCS_DAG_FILE_PATH\n","                        The file path suffix for the Airflow dags.\n"],"name":"stdout"}]},{"metadata":{"id":"y3cVwcERjBAf","colab_type":"text"},"source":["# Setup\n","We set up a few tutorial-essentials in the following cell. These are garbage-collected in the *Cleanup* cell at the end of this notebook.\n","\n","\n","We use Airflow (https://airflow.apache.org/start.html) as the underlying technology for orchestrating and scheduling. To set this up, please first run the \"Airflow Setup\" notebook (under tutorials/BigQuery at https://datalab-alpha.cloud.google.com/docs); it will setup a GCE VM with the Airflow scheduler and the webserver as a long-running process.\n","\n","\n","Running all the cells in the notebook would ensure that there is a VM instance named \"datalab-airflow\" in your project. Note: Without the correct setup of the VM above, the *pipeline* subcommand in the cells below will not work (pipelines will not be deployed)."],"cell_type":"markdown"},{"metadata":{"id":"g1FpZfp7jAhV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["from google.datalab import Context\n","import datetime\n","import google.datalab.bigquery as bq\n","import google.datalab.storage as storage\n","\n","project = Context.default().project_id\n","\n","bucket_name = project + '-bq_pipeline'\n","bucket = storage.Bucket(bucket_name)\n","bucket.create()\n","\n","dataset_name = 'bq_pipeline'\n","dataset = bq.Dataset(dataset_name)\n","dataset.create()\n","\n","gcs_dag_bucket_name = project + '-datalab-airflow'\n","gcs_dag_file_path = 'dags'\n","\n","# Start and end timestamps for our pipelines. \n","start = datetime.datetime.now()\n","formatted_start = start.strftime('%Y%m%dT%H%M%S')\n","end = start + datetime.timedelta(minutes=5)\n","\n","\n"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"VMkRPlgIW783","colab_type":"text"},"source":["# Building a data transformation pipeline\n","The *pipeline* subcommand deploys and orchestrates an ETL pipeline. It supports specifying either an existing BQ table or a GCS path (with accompanying schema) as the data *input*, executing a *transformation* with BQ SQL and producing an *output* of the results (again, either a BQ table or a GCS path). This *pipeline* can be executed on a *schedule*. Additionally, *parameters* can be specified to templatize or customize the pipeline."],"cell_type":"markdown"},{"metadata":{"id":"GtQenup-VZR8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["github_archive = 'githubarchive.month.201801'"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"uWESJgLIMuXm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq query --name my_pull_request_events\n","SELECT id, created_at, repo.name FROM input\n","WHERE actor.login = 'rajivpb' AND type = 'PullRequestEvent'"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"6GdKyfdNM-ti","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# We designate the following 'output' for our pipeline. \n","results_table = project + '.' + dataset_name + '.' + 'pr_events_' + formatted_start\n","\n","# Pipeline name is made unique by suffixing a timestamp\n","pipeline_name = 'github_once_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"eIM5dQm53U9k","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":122},"outputId":"98476caf-6c88-4870-94d3-015d648c3f95","executionInfo":{"status":"ok","timestamp":1516931220828,"user_tz":480,"elapsed":1621,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","input:\n","  table: $github_archive\n","transformation:\n","  query: my_pull_request_events\n","output:\n","  table: $results_table\n","  mode: overwrite\n","schedule:\n","  start: $start\n","  end: $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":25,"outputs":[{"output_type":"stream","text":["[2018-01-26 01:47:00,324] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:00,330] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:00,335] {connectionpool.py:207} INFO - Starting new HTTP connection (1): metadata.google.internal\n","[2018-01-26 01:47:00,382] {connectionpool.py:758} INFO - Starting new HTTPS connection (1): www.googleapis.com\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nimport datetime\\nfrom airflow import DAG\\nfrom airflow.operators.bash_operator import BashOperator\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\\nfrom airflow.contrib.operators.bigquery_to_bigquery import BigQueryToBigQueryOperator\\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\\nfrom airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_load_operator import LoadOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_execute_operator import ExecuteOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_extract_operator import ExtractOperator\\nfrom datetime import timedelta\\n\\ndefault_args = {\\n    \\'owner\\': \\'Google Cloud Datalab\\',\\n    \\'email\\': [],\\n    \\'start_date\\': datetime.datetime.strptime(\\'2018-01-26T01:46:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n    \\'end_date\\': datetime.datetime.strptime(\\'2018-01-26T01:51:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n}\\n\\ndag = DAG(dag_id=\\'github_once_20180126T014654\\', schedule_interval=\\'@once\\', catchup=True, default_args=default_args)\\n\\nbq_pipeline_execute_task = ExecuteOperator(task_id=\\'bq_pipeline_execute_task_id\\', mode=\"\"\"overwrite\"\"\", parameters=None, sql=\"\"\"WITH input AS (\\n  SELECT * FROM `githubarchive.month.201801`\\n)\\n\\nSELECT id, created_at, repo.name FROM input\\nWHERE actor.login = \\'rajivpb\\' AND type = \\'PullRequestEvent\\'\"\"\", table=\"\"\"cloud-ml-dev.bq_pipeline.pr_events_20180126T014654\"\"\", dag=dag)\\n'"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"ezMr2phDJZ_T","colab_type":"text"},"source":["When the above cell is run, a pipeline is deployed and the results of the query are written into the BQ results table (i.e. $results_table). It could take 5-10 min between when the cell is executed for the result_table to show up. Below, we'll see additional examples for alternate ways of specifying the source, the source-types supported, and for customizing the pipeline."],"cell_type":"markdown"},{"metadata":{"id":"qE1wOSrsMQbB","colab_type":"text"},"source":["# Parameterization\n","The *parameters* section provides the ability to customize the inputs and outputs of the pipeline. These parameters are merged with the SQL query parameters into a list, and are specified in the cell body (along the same lines as the *%bq execute* command, for example). \n","\n","In addition to parameters that the users can define, the following mapping keys have been made available for formatting strings and are designed to capture common scenarios around parameterizing the pipeline with the execution timestamp. \n","\n"," - '_ds': the date formatted as YYYY-MM-DD\n"," - '_ts': the full ISO-formatted timestamp YYYY-MM-DDTHH:MM:SS.mmmmmm\n"," - '_ds_nodash': the date formatted as YYYYMMDD (i.e. YYYY-MM-DD with 'no dashes')\n"," - '_ts_nodash': the timestamp formatted as YYYYMMDDTHHMMSSmmmmmm (i.e full ISO-formatted timestamp without dashes or colons)\n"," - '_ts_year': 4-digit year\n"," - '_ts_month': '1'-'12'\n"," - '_ts_day': '1'-'31'\n"," - '_ts_hour': '0'-'23'\n"," - '_ts_minute': '0'-'59'\n"," - '_ts_second': '0'-'59'\n"],"cell_type":"markdown"},{"metadata":{"id":"jLUJaAzbpyuQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# The source/input is formatted with the built-in mapping keys _ts_year and \n","# _ts_month and these are evaluated (or \"bound\") at the time of pipeline \n","# execution. This could be at some point in the future, or at some point in the \n","# \"past\" in cases where a backfill job is being executed.\n","github_archive_current_month = 'githubarchive.month.%(_ts_year)s%(_ts_month)s'\n","\n","# The destination/output is formatted with additional user-defined parameters\n","# 'project', 'dataset', and 'user'. These are evaluated/bound at the time of \n","# execution of the %bq pipeline cell. \n","results_table = '%(project)s.%(dataset_name)s.%(user)s_pr_events_%(_ts_nodash)s'\n","\n","pipeline_name = 'github_parameterized_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"OeAJ6o6swS78","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq query --name my_pull_request_events\n","SELECT id, created_at, repo.name FROM input\n","WHERE actor.login = @user AND type = 'PullRequestEvent'\n"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"-_3F4BdjqQzz","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":122},"outputId":"c917d76b-de53-465e-99cd-7eafdc5188de","executionInfo":{"status":"ok","timestamp":1516931224599,"user_tz":480,"elapsed":1273,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","input:\n","  table: $github_archive_current_month\n","transformation:\n","  query: my_pull_request_events\n","output:\n","  table: $results_table\n","  mode: overwrite\n","parameters:\n","  - name: user\n","    type: STRING\n","    value: 'rajivpb'\n","  - name: project\n","    type: STRING\n","    value: $project\n","  - name: dataset_name\n","    type: STRING\n","    value: $dataset_name\n","schedule:\n","  start: $start\n","  end: $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":28,"outputs":[{"output_type":"stream","text":["[2018-01-26 01:47:04,047] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:04,054] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:04,059] {connectionpool.py:207} INFO - Starting new HTTP connection (1): metadata.google.internal\n","[2018-01-26 01:47:04,106] {connectionpool.py:758} INFO - Starting new HTTPS connection (1): www.googleapis.com\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nimport datetime\\nfrom airflow import DAG\\nfrom airflow.operators.bash_operator import BashOperator\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\\nfrom airflow.contrib.operators.bigquery_to_bigquery import BigQueryToBigQueryOperator\\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\\nfrom airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_load_operator import LoadOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_execute_operator import ExecuteOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_extract_operator import ExtractOperator\\nfrom datetime import timedelta\\n\\ndefault_args = {\\n    \\'owner\\': \\'Google Cloud Datalab\\',\\n    \\'email\\': [],\\n    \\'start_date\\': datetime.datetime.strptime(\\'2018-01-26T01:46:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n    \\'end_date\\': datetime.datetime.strptime(\\'2018-01-26T01:51:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n}\\n\\ndag = DAG(dag_id=\\'github_parameterized_20180126T014654\\', schedule_interval=\\'@once\\', catchup=True, default_args=default_args)\\n\\nbq_pipeline_execute_task = ExecuteOperator(task_id=\\'bq_pipeline_execute_task_id\\', mode=\"\"\"overwrite\"\"\", parameters=[{\\'type\\': \\'STRING\\', \\'name\\': \\'user\\', \\'value\\': \\'rajivpb\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'project\\', \\'value\\': \\'cloud-ml-dev\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'dataset_name\\', \\'value\\': \\'bq_pipeline\\'}], sql=\"\"\"WITH input AS (\\n  SELECT * FROM `githubarchive.month.{{ \\'{:04d}\\'.format(execution_date.year) }}{{ \\'{:02d}\\'.format(execution_date.month) }}`\\n)\\n\\nSELECT id, created_at, repo.name FROM input\\nWHERE actor.login = @user AND type = \\'PullRequestEvent\\'\"\"\", table=\"\"\"cloud-ml-dev.bq_pipeline.rajivpb_pr_events_{{ ts_nodash }}\"\"\", dag=dag)\\n'"]},"metadata":{"tags":[]},"execution_count":28}]},{"metadata":{"id":"81_9F7vqe7Hm","colab_type":"text"},"source":["# SQL-based data transformation pipeline for GCS data\n","*pipeline* also supports specifying GCS paths as both the input (accompanied by a schema) and output, thus completely bypassing the specification of any BQ tables. Garbage collection of all intermediate BQ tables is handled for the user."],"cell_type":"markdown"},{"metadata":{"id":"m8Kx_Unj-E1V","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["gcs_input_path = 'gs://cloud-datalab-samples/cars.csv'\n","gcs_output_path = 'gs://%(bucket_name)s/all_makes_%(_ts_nodash)s.csv'\n","pipeline_name = 'gcs_to_gcs_transform_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"11R6SnbSOokH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["%%bq query --name all_makes\n","SELECT Make FROM input"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"xqGHWFlLex_V","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":122},"outputId":"5dcd5717-a566-413f-e9e4-29f8a46048c9","executionInfo":{"status":"ok","timestamp":1516931228726,"user_tz":480,"elapsed":1286,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","input:\n","  path: $gcs_input_path\n","  schema:\n","    - name: Year\n","      type: INTEGER\n","    - name: Make\n","      type: STRING\n","    - name: Model\n","      type: STRING\n","    - name: Description\n","      type: STRING\n","    - name: Price\n","      type: FLOAT\n","  csv:\n","    skip: 1\n","transformation: \n","  query: all_makes\n","output:\n","  path: $gcs_output_path\n","parameters:\n","  - name: bucket_name\n","    type: STRING\n","    value: $bucket_name\n","schedule:\n","  start: $start\n","  end:  $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":31,"outputs":[{"output_type":"stream","text":["[2018-01-26 01:47:08,213] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:08,219] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:08,222] {connectionpool.py:207} INFO - Starting new HTTP connection (1): metadata.google.internal\n","[2018-01-26 01:47:08,267] {connectionpool.py:758} INFO - Starting new HTTPS connection (1): www.googleapis.com\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nimport datetime\\nfrom airflow import DAG\\nfrom airflow.operators.bash_operator import BashOperator\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\\nfrom airflow.contrib.operators.bigquery_to_bigquery import BigQueryToBigQueryOperator\\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\\nfrom airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_load_operator import LoadOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_execute_operator import ExecuteOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_extract_operator import ExtractOperator\\nfrom datetime import timedelta\\n\\ndefault_args = {\\n    \\'owner\\': \\'Google Cloud Datalab\\',\\n    \\'email\\': [],\\n    \\'start_date\\': datetime.datetime.strptime(\\'2018-01-26T01:46:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n    \\'end_date\\': datetime.datetime.strptime(\\'2018-01-26T01:51:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n}\\n\\ndag = DAG(dag_id=\\'gcs_to_gcs_transform_20180126T014654\\', schedule_interval=\\'@once\\', catchup=True, default_args=default_args)\\n\\nbq_pipeline_execute_task = ExecuteOperator(task_id=\\'bq_pipeline_execute_task_id\\', csv_options={\\'skip\\': 1}, data_source=\"\"\"input\"\"\", parameters=[{\\'type\\': \\'STRING\\', \\'name\\': \\'bucket_name\\', \\'value\\': \\'cloud-ml-dev-bq_pipeline\\'}], path=\"\"\"gs://cloud-datalab-samples/cars.csv\"\"\", schema=[{\\'type\\': \\'INTEGER\\', \\'name\\': \\'Year\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'Make\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'Model\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'Description\\'}, {\\'type\\': \\'FLOAT\\', \\'name\\': \\'Price\\'}], sql=\"\"\"SELECT Make FROM input\"\"\", dag=dag)\\nbq_pipeline_extract_task = ExtractOperator(task_id=\\'bq_pipeline_extract_task_id\\', path=\"\"\"gs://cloud-ml-dev-bq_pipeline/all_makes_{{ ts_nodash }}.csv\"\"\", table=\"\"\"{{ ti.xcom_pull(task_ids=\\'bq_pipeline_execute_task_id\\').get(\\'table\\') }}\"\"\", dag=dag)\\nbq_pipeline_extract_task.set_upstream(bq_pipeline_execute_task)\\n'"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"16QvAy_WSyDn","colab_type":"text"},"source":["# Load data from GCS into BigQuery\n","*pipeline* can also be used to parameterize and schedule the loading of data from GCS to BQ, i.e the equivalent of the *%bq load* command."],"cell_type":"markdown"},{"metadata":{"id":"RVwTNF4jnGA7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["bq_load_results_table = '%(project)s.%(dataset_name)s.cars_load_%(_ts_nodash)s'\n","pipeline_name = 'load_gcs_to_bq_' + formatted_start"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"-qHQ4bfkXwEU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":122},"outputId":"a7fe7672-1d5e-4cc6-c6df-3f1b634bd8dd","executionInfo":{"status":"ok","timestamp":1516931231540,"user_tz":480,"elapsed":1377,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","load:\n","  path: $gcs_input_path\n","  schema:\n","    - name: Year\n","      type: INTEGER\n","    - name: Make\n","      type: STRING\n","    - name: Model\n","      type: STRING\n","    - name: Description\n","      type: STRING\n","    - name: Price\n","      type: FLOAT\n","  csv:\n","    skip: 1\n","  table: $bq_load_results_table\n","  mode: overwrite\n","parameters:\n","  - name: project\n","    type: STRING\n","    value: $project\n","  - name: dataset_name\n","    type: STRING\n","    value: $dataset_name\n","schedule:\n","  start: $start\n","  end: $end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":33,"outputs":[{"output_type":"stream","text":["[2018-01-26 01:47:11,013] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:11,019] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:11,023] {connectionpool.py:207} INFO - Starting new HTTP connection (1): metadata.google.internal\n","[2018-01-26 01:47:11,069] {connectionpool.py:758} INFO - Starting new HTTPS connection (1): www.googleapis.com\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nimport datetime\\nfrom airflow import DAG\\nfrom airflow.operators.bash_operator import BashOperator\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\\nfrom airflow.contrib.operators.bigquery_to_bigquery import BigQueryToBigQueryOperator\\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\\nfrom airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_load_operator import LoadOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_execute_operator import ExecuteOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_extract_operator import ExtractOperator\\nfrom datetime import timedelta\\n\\ndefault_args = {\\n    \\'owner\\': \\'Google Cloud Datalab\\',\\n    \\'email\\': [],\\n    \\'start_date\\': datetime.datetime.strptime(\\'2018-01-26T01:46:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n    \\'end_date\\': datetime.datetime.strptime(\\'2018-01-26T01:51:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n}\\n\\ndag = DAG(dag_id=\\'load_gcs_to_bq_20180126T014654\\', schedule_interval=\\'@once\\', catchup=True, default_args=default_args)\\n\\nbq_pipeline_load_task = LoadOperator(task_id=\\'bq_pipeline_load_task_id\\', csv_options={\\'skip\\': 1}, mode=\"\"\"overwrite\"\"\", path=\"\"\"gs://cloud-datalab-samples/cars.csv\"\"\", schema=[{\\'type\\': \\'INTEGER\\', \\'name\\': \\'Year\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'Make\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'Model\\'}, {\\'type\\': \\'STRING\\', \\'name\\': \\'Description\\'}, {\\'type\\': \\'FLOAT\\', \\'name\\': \\'Price\\'}], table=\"\"\"cloud-ml-dev.bq_pipeline.cars_load_{{ ts_nodash }}\"\"\", dag=dag)\\n'"]},"metadata":{"tags":[]},"execution_count":33}]},{"metadata":{"id":"cnuNrYDCXiXT","colab_type":"text"},"source":["# Extract data from BigQuery into GCS\n","Similar to load, *pipeline* can also be used to perform the equivalent of the *%bq extract* command."],"cell_type":"markdown"},{"metadata":{"id":"nWgnLdX9YH7e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# This pipeline depends on the output table from the previous pipeline (i.e. the \n","# \"load\") pipeline. So we have this start a wee bit later.\n","later_start = start + datetime.timedelta(minutes=5)\n","later_end = later_start + datetime.timedelta(minutes=5)\n","\n","gcs_extract_path = 'gs://%(bucket_name)s/cars_extract_%(_ts_nodash)s.csv'\n","pipeline_name = 'extract_bq_to_gcs_' + later_start.strftime('%Y%m%dT%H%M%S')"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"uu1YuNJIFpTM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":122},"outputId":"957831f9-085b-4a92-f8ee-3da1aa8c7a69","executionInfo":{"status":"ok","timestamp":1516931234651,"user_tz":480,"elapsed":1511,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["%%bq pipeline --name $pipeline_name -d $gcs_dag_bucket_name -f $gcs_dag_file_path\n","extract:\n","  table: $bq_load_results_table\n","  path: $gcs_extract_path\n","  format: csv\n","  csv:\n","    delimiter: '#'\n","parameters:\n","  - name: bucket_name\n","    type: STRING\n","    value: $bucket_name\n","  - name: project\n","    type: STRING\n","    value: $project\n","  - name: dataset_name\n","    type: STRING\n","    value: $dataset_name\n","schedule:\n","  start: $later_start\n","  end:  $later_end\n","  interval: '@once'\n","  catchup: True"],"cell_type":"code","execution_count":35,"outputs":[{"output_type":"stream","text":["[2018-01-26 01:47:14,057] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:14,064] {_default.py:280} WARNING - No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n","[2018-01-26 01:47:14,068] {connectionpool.py:207} INFO - Starting new HTTP connection (1): metadata.google.internal\n","[2018-01-26 01:47:14,115] {connectionpool.py:758} INFO - Starting new HTTPS connection (1): www.googleapis.com\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nimport datetime\\nfrom airflow import DAG\\nfrom airflow.operators.bash_operator import BashOperator\\nfrom airflow.contrib.operators.bigquery_operator import BigQueryOperator\\nfrom airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator\\nfrom airflow.contrib.operators.bigquery_to_bigquery import BigQueryToBigQueryOperator\\nfrom airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator\\nfrom airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_load_operator import LoadOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_execute_operator import ExecuteOperator\\nfrom google.datalab.contrib.bigquery.operators._bq_extract_operator import ExtractOperator\\nfrom datetime import timedelta\\n\\ndefault_args = {\\n    \\'owner\\': \\'Google Cloud Datalab\\',\\n    \\'email\\': [],\\n    \\'start_date\\': datetime.datetime.strptime(\\'2018-01-26T01:46:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n    \\'end_date\\': datetime.datetime.strptime(\\'2018-01-26T01:51:54\\', \\'%Y-%m-%dT%H:%M:%S\\'),\\n}\\n\\ndag = DAG(dag_id=\\'extract_bq_to_gcs_20180126T014654\\', schedule_interval=\\'@once\\', catchup=True, default_args=default_args)\\n\\nbq_pipeline_extract_task = ExtractOperator(task_id=\\'bq_pipeline_extract_task_id\\', csv_options={\\'delimiter\\': \\'#\\'}, format=\"\"\"csv\"\"\", path=\"\"\"gs://cloud-ml-dev-bq_pipeline/cars_extract_{{ ts_nodash }}.csv\"\"\", table=\"\"\"cloud-ml-dev.bq_pipeline.cars_load_{{ ts_nodash }}\"\"\", dag=dag)\\n'"]},"metadata":{"tags":[]},"execution_count":35}]},{"metadata":{"id":"kyph2e8WhlCf","colab_type":"text"},"source":["#Cleanup"],"cell_type":"markdown"},{"metadata":{"id":"G7MApPqchoul","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["# Delete the contents of the GCS bucket, the GCS bucket itself, and the BQ \n","# dataset. Uncomment the lines and execute them.\n","#!gsutil rm -r gs://$bucket_name\n","#!gsutil rb gs://$bucket_name\n","#!bq rm -r -f $dataset_name"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"YU5SK7_lj7_S","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"efcc6b7a-51aa-4869-8d7e-ff793520c175","executionInfo":{"status":"ok","timestamp":1516931244354,"user_tz":480,"elapsed":8112,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["# This just verifies that cleanup actually worked. \n","\n","#Should show an error message like BucketNotFoundException: 404 gs://...\n","!gsutil ls gs://$bucket_name\n","!gsutil ls gs://$gcs_dag_bucket_name\n","  \n","#Should show an error message like BigQuery error in ls operation: Not found ...\n","!bq ls $dataset_name\n"],"cell_type":"code","execution_count":37,"outputs":[{"output_type":"stream","text":["gs://cloud-ml-dev-datalab-airflow/dags/\r\n"],"name":"stdout"}]}]}