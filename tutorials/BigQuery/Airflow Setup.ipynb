{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Airflow Setup.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"3WSgPBsGt2aW","colab_type":"text"},"source":["# Setup"],"cell_type":"markdown"},{"metadata":{"id":"v4UMnOJJng1s","colab_type":"text"},"source":["This notebook creates a VM in the user's project with the airflow scheduler and webserver. A default GCP zone for the VM has been chosen (below). Feel free to change this as desired."],"cell_type":"markdown"},{"metadata":{"id":"cFjQUmz5jIRT","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["zone='us-central1-b'"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"ppy40Vihk1xk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":34},"outputId":"dba10e38-60db-4d0f-ed5a-a85dde6f58da","executionInfo":{"status":"ok","timestamp":1516994224304,"user_tz":480,"elapsed":3332,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["from google.datalab import Context\n","import google.datalab.storage as storage\n","\n","project = Context.default().project_id\n","vm_name = 'datalab-airflow-1'\n","\n","# The name of this GCS bucket follows a convention between this notebook and \n","# the 'BQ Pipeline' tutorial notebook; so don't change this!\n","gcs_dag_bucket_name = project + '-' + vm_name\n","gcs_dag_bucket = storage.Bucket(gcs_dag_bucket_name)\n","gcs_dag_bucket.create()"],"cell_type":"code","execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Google Cloud Storage Bucket gs://cloud-ml-dev-datalab-airflow-1"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"9HivgAXasXyN","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["vm_startup_script_contents = \"\"\"#!/bin/bash\n","apt-get update\n","apt-get --assume-yes install python-pip\n","\n","# TODO(rajivpb): Replace this with \"pip install datalab\"\n","DATALAB_TAR=datalab-1.1.0.tar\n","gsutil cp gs://datalab-pipelines/$DATALAB_TAR $DATALAB_TAR\n","pip install $DATALAB_TAR\n","rm $DATALAB_TAR\n","\n","pip install apache-airflow==1.9.0\n","pip install pandas-gbq==0.3.0\n","\n","export AIRFLOW_HOME=/airflow\n","export AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False\n","export AIRFLOW__CORE__LOAD_EXAMPLES=False\n","airflow initdb\n","airflow scheduler &\n","airflow webserver -p 8080 &\n","\n","# We append a gsutil rsync command to the cron file and have this run every minute to sync dags.\n","PROJECT_ID=$(gcloud info --format=\"get(config.project)\")\n","GCS_DAG_BUCKET=$PROJECT_ID-datalab-airflow\n","AIRFLOW_CRON=temp_crontab.txt\n","crontab -l > $AIRFLOW_CRON\n","DAG_FOLDER=\"dags\"\n","LOCAL_DAG_PATH=$AIRFLOW_HOME/$DAG_FOLDER\n","mkdir $LOCAL_DAG_PATH\n","echo \"* * * * * gsutil rsync gs://$GCS_DAG_BUCKET/$DAG_FOLDER $LOCAL_DAG_PATH\" >> $AIRFLOW_CRON\n","crontab $AIRFLOW_CRON\n","rm $AIRFLOW_CRON\n","EOF\n","\"\"\"\n","vm_startup_script_file_name = 'vm_startup_script.sh'\n","script_file = open(vm_startup_script_file_name, 'w')\n","script_file.write(vm_startup_script_contents)\n","script_file.close()"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"B-3e418B6awV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":68},"outputId":"8a6b7d0a-9aeb-4306-95c5-f3aef86e0de3","executionInfo":{"status":"ok","timestamp":1516994245592,"user_tz":480,"elapsed":19873,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["import subprocess\n","print subprocess.check_output([\n","    'gcloud', 'beta', 'compute', '--project', project, 'instances', 'create', vm_name, \n","    '--zone', zone,\n","    '--machine-type', 'n1-standard-1',\n","    '--network', 'default',\n","    '--maintenance-policy', 'MIGRATE',\n","    '--scopes', 'https://www.googleapis.com/auth/cloud-platform',\n","    '--image', 'debian-9-stretch-v20171025',\n","    '--image-project', 'debian-cloud',\n","    '--boot-disk-size', '10',\n","    '--boot-disk-type', 'pd-standard',\n","    '--boot-disk-device-name', vm_name,\n","    '--metadata-from-file', 'startup-script=' + vm_startup_script_file_name])"],"cell_type":"code","execution_count":16,"outputs":[{"output_type":"stream","text":["NAME               ZONE           MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUS\n","datalab-airflow-1  us-central1-b  n1-standard-1               10.240.0.14  35.192.219.140  RUNNING\n","\n"],"name":"stdout"}]},{"metadata":{"id":"p9QSZ9J5twi0","colab_type":"text"},"source":["# Cleanup\n"],"cell_type":"markdown"},{"metadata":{"id":"HtswWJqttq99","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"source":["#The following cleans up the VM and associated GCS bucket. Uncomment and run.\n","#!gsutil rm -r gs://$gcs_dag_bucket_name\n","#!gsutil rb gs://$gcs_dag_bucket_name\n","#!gcloud compute instances delete datalab-airflow --zone us-central1-b --quiet"],"cell_type":"code","execution_count":0,"outputs":[]},{"metadata":{"id":"tg1Hl1ivH2yb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":119},"outputId":"de867c6c-f59f-43aa-c02f-a6435c98b2dd","executionInfo":{"status":"ok","timestamp":1516994250065,"user_tz":480,"elapsed":3090,"user":{"displayName":"Rajiv Bharadwaja","userId":"114213070630621217486"}}},"source":["# This just verifies that cleanup actually worked. Should show an error like \n","# \"BucketNotFoundException: 404 ...\". \n","!gsutil ls gs://$gcs_dag_bucket_name"],"cell_type":"code","execution_count":18,"outputs":[{"output_type":"stream","text":["\r\n","\r\n","Updates are available for some Cloud SDK components.  To install them,\r\n","please run:\r\n","  $ gcloud components update\r\n","\r\n"],"name":"stdout"}]}]}